{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUHdax+WbBGkkR0oj1dJkI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enya-yx/LangChain-Courses/blob/main/langgraph_stream_persistency_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain-google-genai\" \"langchain\" \"langchain-core\" \"langgraph-prebuilt\" \"google-generativeai\" \"langchain_community\" \"docarray\" \"langchain_experimental\" \"aiosqlite\""
      ],
      "metadata": {
        "id": "sIdyIP6YPtKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('google_api_key')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily_api_key')\n",
        "\n",
        "# Configure the generative AI library with your API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "6gPZp3CQXQzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "#from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define llm\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "UL4c9YZWO0X7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Define a Simple Agent State;\n",
        "# Agent State is accessible to all parts of the graph.\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list[AnyMessage], operator.add]"
      ],
      "metadata": {
        "id": "EnxFL8gX_nzr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearchResults(max_results=2)\n",
        "print(tool.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xZ9ErqeMbpx",
        "outputId": "a06bb0d7-36fd-4248-b0b1-757e94cc9287"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tavily_search_results_json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "G16mz_zHR6dW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple agent\n",
        "class Agent:\n",
        "  def __init__(self, model, checkpointer, tools ,system=\"\"):\n",
        "    self.system = system\n",
        "    graph = StateGraph(AgentState)\n",
        "    graph.add_node(\"llm\",self.call_llm)\n",
        "    graph.add_node(\"action\", self.take_action)\n",
        "    graph.add_conditional_edges(\n",
        "        \"llm\",\n",
        "        self.exists_action,\n",
        "        {True: \"action\", False: END}\n",
        "    )\n",
        "    graph.add_edge(\"action\", \"llm\")\n",
        "    graph.set_entry_point(\"llm\")\n",
        "    self.graph = graph.compile(checkpointer = checkpointer)\n",
        "    self.tools = {t.name: t for t in tools}\n",
        "    self.model = model.bind_tools(tools)\n",
        "\n",
        "  def exists_action(self, state: AgentState):\n",
        "    result = state['messages'][-1]\n",
        "    return len(result.tool_calls) > 0\n",
        "\n",
        "  def call_llm(self, state: AgentState):\n",
        "    messages = state[\"messages\"]\n",
        "    if self.system:\n",
        "      messages = [SystemMessage(content=self.system)] + messages\n",
        "    message = self.model.invoke(messages)\n",
        "    next_tools = message.tool_calls if message.tool_calls else [\"END\"]\n",
        "    #print(f\"Next tools: {next_tools}\")\n",
        "    return {'messages': [message]}\n",
        "\n",
        "  def take_action(self, state: AgentState):\n",
        "    tool_calls = state['messages'][-1].tool_calls\n",
        "    results = []\n",
        "    for t in tool_calls:\n",
        "      print(f\"Calling function: {t}\")\n",
        "      result = self.tools[t['name']].invoke(t['args'])\n",
        "      print(f\"Result from calling function: \", result)\n",
        "      results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
        "    print(\"Back to the model\")\n",
        "    return {'messages': results}"
      ],
      "metadata": {
        "id": "hcOlLNKWAzSq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are a smart research assistant. Use the search engine to query results. \\\n",
        "You are allowed to make multiple calls (either together or in sequence).\\\n",
        "Only look up information when you are sure of what you want. \\\n",
        "\"\"\"\n",
        "abot = Agent(llm, memory, [tool], system=prompt)"
      ],
      "metadata": {
        "id": "zTEn9IfqDnI8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(content = \"What is the weather in Ann Arbor, MI\")]\n",
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "i = 1\n",
        "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
        "  print(\"step-\", i)\n",
        "  for v in event.values():\n",
        "    print(v)\n",
        "  i += 1"
      ],
      "metadata": {
        "id": "7SeyjY5KFPXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages2 = [HumanMessage(content=\"What is the weather in SF\")]\n",
        "for event in abot.graph.stream({\"messages\": messages2}, thread):\n",
        "  for v in event.values():\n",
        "    print(v)\n"
      ],
      "metadata": {
        "id": "24gzF-u4L1vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages3 = [HumanMessage(content=\"Which one is the warmer\")]\n",
        "for event in abot.graph.stream({\"messages\": messages3}, thread):\n",
        "  for v in event.values():\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "qupI_72IkZdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pending: apply async stream call to print each token"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cKYqvZJ6kzug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}