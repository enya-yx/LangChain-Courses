{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8mAH7jbDaH335ptekJnQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enya-yx/LangChain-Courses/blob/main/tool_and_routing_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain-google-genai\" \"langchain\" \"langchain-core\" \"langgraph-prebuilt\" \"google-generativeai\" \"langchain_community\" \"docarray\" \"langchain_experimental\""
      ],
      "metadata": {
        "id": "sIdyIP6YPtKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('google_api_key')\n",
        "# Configure the generative AI library with your API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "6gPZp3CQXQzM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define llm\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "UL4c9YZWO0X7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tools by decorator and using pandatic class as args\n",
        "from typing import List\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain.output_parsers import PydanticOutputParser\n",
        "from langchain.agents import tool\n",
        "\n",
        "class SearchInput(BaseModel):\n",
        "  query: str = Field(description=\"thing to search for\")\n",
        "\n",
        "@tool(args_schema=SearchInput)\n",
        "def search(query: str) -> str:\n",
        "  \"\"\"Search for weather online\"\"\"\n",
        "  return \"42f\"\n",
        "search.args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAw3N08n_F08",
        "outputId": "d93c80e7-bc09-4cd9-9593-451fcc66172f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': {'description': 'search query', 'title': 'Query', 'type': 'string'}}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the first tool to query current temperature for a position\n",
        "import requests\n",
        "class Position(BaseModel):\n",
        "  latitude: float = Field(description=\"latitude of the location\")\n",
        "  longitude: float = Field(description=\"longitude of the location\")\n",
        "\n",
        "@tool(args_schema=Position)\n",
        "def get_current_temperature(latitude: float, longitude: float) -> str:\n",
        "  \"\"\"Get the current temperature\"\"\"\n",
        "\n",
        "  BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
        "  params = {\n",
        "    \"latitude\": latitude,\n",
        "    \"longitude\": longitude,\n",
        "    \"hourly\": \"temperature_2m\",\n",
        "    \"forecast_days\": 1,\n",
        "  }\n",
        "  response = requests.get(BASE_URL, params=params)\n",
        "  if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    hourly_data = data[\"hourly\"]\n",
        "    temperature_2m = hourly_data[\"temperature_2m\"]\n",
        "    unit = data[\"hourly_units\"][\"temperature_2m\"]\n",
        "\n",
        "    return f\"The current temperature is: {temperature_2m[0]}{unit}\"\n",
        "  else:\n",
        "    raise Exception(f\"Request failed with status code {response.status_code}\")\n",
        "\n",
        "\n",
        "get_current_temperature.invoke({\"latitude\": 13, \"longitude\":14})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DP6W12QvAzPe",
        "outputId": "a89cceb5-919b-4da8-a120-195a8edf14e8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is: 24.4°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.render import format_tool_to_openai_function\n",
        "get_current_temperature_function = format_tool_to_openai_function(get_current_temperature)\n",
        "get_current_temperature_function\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kg5dl_7eCsJn",
        "outputId": "c2179b5a-41df-4e4b-ba94-7ac75814c886"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': 'get_current_temperature',\n",
              " 'description': 'Get the current temperature',\n",
              " 'parameters': {'properties': {'latitude': {'description': 'latitude of the location',\n",
              "    'type': 'number'},\n",
              "   'longitude': {'description': 'longitude of the location',\n",
              "    'type': 'number'}},\n",
              "  'required': ['latitude', 'longitude'],\n",
              "  'type': 'object'}}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define prompt and bind function to llm\n",
        "template_string = \"What is the current temperature at position latitude: {latitude}, longitude: {longitude}?\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables = [\"latitude\", \"longitude\"],\n",
        "    template=template_string\n",
        ")\n",
        "#message_test = prompt.format(latitude=13, longitude=14)\n",
        "#message_test\n",
        "\n",
        "llm_with_function = llm.bind(functions = [get_current_temperature_function])\n",
        "\n",
        "chain = prompt | llm_with_function\n",
        "res = chain.invoke({\"latitude\": 13, \"longitude\":14})\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjh4pOk5GKW-",
        "outputId": "aea682de-9873-409f-cd43-8b0c9549b60b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'get_current_temperature', 'arguments': '{\"latitude\": 13.0, \"longitude\": 14.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--3a5931bc-ce93-45c1-ba9d-663f88892a47-0' tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 13.0, 'longitude': 14.0}, 'id': '8b8e9bd9-442a-49d8-88cf-ff3bf10c2ba6', 'type': 'tool_call'}] usage_metadata={'input_tokens': 75, 'output_tokens': 24, 'total_tokens': 177, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "tYaAE26PT5-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the second tool to use wikipedia\n",
        "import wikipedia\n",
        "@tool\n",
        "def search_wikipedia(query: str) -> str:\n",
        "  \"\"\"Search wikipedia for the given query and get page summaries\"\"\"\n",
        "  page_titles = wikipedia.search(query)\n",
        "  summaries = []\n",
        "  for page_title in page_titles[:3]:\n",
        "    try:\n",
        "      wiki_pedia_page = wikipedia.page(page_title)\n",
        "      summaries.append(f\"Page: {page_title}\\nSummary: {wiki_pedia_page.summary}\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "      print(f\"Skipping disambiguation page: {page_title}\")\n",
        "\n",
        "  if not summaries:\n",
        "    return \"No good Wikipedia Search Result was found\"\n",
        "\n",
        "  return \"\\n\\n\".join(summaries)\n",
        "\n",
        "search_wikipedia_function = format_tool_to_openai_function(search_wikipedia)"
      ],
      "metadata": {
        "id": "6HQFmxVtSDrF"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bind model with multiple functions\n",
        "llm_multi_function = llm.bind(functions = [get_current_temperature_function, search_wikipedia_function])\n",
        "res1 = llm_multi_function.invoke(\"What is the current weather at the position: latitude 13, longitide 14 right now?\")\n",
        "res2 = llm_multi_function.invoke(\"Give a short introduction about the game Werewolves Kill\")\n",
        "print(res1)\n",
        "print(res2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLzMAT8VTHAL",
        "outputId": "a0384399-5eac-4a7a-e90b-5e1a3e14bcd1"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='' additional_kwargs={'function_call': {'name': 'get_current_temperature', 'arguments': '{\"latitude\": 13.0, \"longitude\": 14.0}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--b5c68c90-c565-4ccd-bdb7-4faca86fd95c-0' tool_calls=[{'name': 'get_current_temperature', 'args': {'latitude': 13.0, 'longitude': 14.0}, 'id': 'dd9d1df3-3000-4a6d-bcb7-afb7d3520e0b', 'type': 'tool_call'}] usage_metadata={'input_tokens': 121, 'output_tokens': 24, 'total_tokens': 233, 'input_token_details': {'cache_read': 0}}\n",
            "content='' additional_kwargs={'function_call': {'name': 'search_wikipedia', 'arguments': '{\"query\": \"Werewolves Kill game\"}'}} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run--cb14cd93-b04b-420f-a10c-54723353c53e-0' tool_calls=[{'name': 'search_wikipedia', 'args': {'query': 'Werewolves Kill game'}, 'id': 'cbe1efba-237f-4bf5-b554-03de338a05ca', 'type': 'tool_call'}] usage_metadata={'input_tokens': 108, 'output_tokens': 18, 'total_tokens': 181, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40cc0b11",
        "outputId": "7942ece2-75bc-439f-be82-ea634554681f"
      },
      "source": [
        "# Define the executor as a rounter to trigger tools or return the content\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "def tool_executor(message: BaseMessage):\n",
        "    if message.tool_calls:\n",
        "        tool_call = message.tool_calls[0]\n",
        "        tool_name = tool_call['name']\n",
        "        tool_args = tool_call['args']\n",
        "        tools = {\n",
        "          \"get_current_temperature\": get_current_temperature,\n",
        "          \"search_wikipedia\": search_wikipedia\n",
        "        }\n",
        "\n",
        "        # Assuming get_current_temperature is accessible in the scope\n",
        "        if tool_name in tools:\n",
        "            return tools[tool_name].invoke(tool_args)\n",
        "        else:\n",
        "            # Handle other tools if necessary, or raise an error\n",
        "            return f\"Unknown tool: {tool_name}\"\n",
        "    else:\n",
        "        return message.content # Return the LLM's content if no tool call\n",
        "\n",
        "tool_execution_runnable = RunnableLambda(tool_executor)\n",
        "\n",
        "print(\"Defined tool_executor function and tool_execution_runnable.\")"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined tool_executor function and tool_execution_runnable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6f0b5cf9",
        "outputId": "fec5125f-798c-4566-9625-efed90e227fc"
      },
      "source": [
        "# Add tool executor to the chain to trigger the function automatically\n",
        "# from langchain.agents.output_parsers import JsonOutputFunctionsParser\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"{input}\")]\n",
        ")\n",
        "\n",
        "full_chain = prompt | llm_multi_function | tool_execution_runnable"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current temperature is: 24.4°C'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"input\": \"What is the current weather at the position: latitude 13, longitide 14 right now?\"})"
      ],
      "metadata": {
        "id": "TGGhEcJ4pY_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"input\": \"Hi, I'm Yx\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fLNrEAMRjaDb",
        "outputId": "a9e178fe-1450-403f-bd9b-6f16f8719690"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello Yx! Nice to meet you. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_chain.invoke({\"input\": \"What is the game of Werewolves Kill?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "arEX4R9wpaj7",
        "outputId": "430416f9-cf03-481c-cc8f-7cc601e7ba00"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Page: Mafia (party game)\\nSummary: Mafia, also known as Werewolf, is a social deduction game created in 1986 by Dimitry Davidoff, then a psychology student at Moscow State University. The game models a conflict between two groups: an informed minority (the mafiosi or the werewolves) and an uninformed majority (the villagers). At the start of the game, each player is secretly assigned a role affiliated with one of these teams. The game has two alternating phases: first, a night-phase, during which those with night-killing-powers may covertly kill other players, and second, a day-phase, in which all surviving players debate and vote to eliminate a suspect. The game continues until a faction achieves its win condition; for the village, this usually means eliminating the evil minority, while for the minority, this usually means reaching numerical parity with the village and eliminating any rival evil groups.\\n\\n\\n\\nPage: Werewolves Within (film)\\nSummary: Werewolves Within is a 2021 American comedy horror film directed by Josh Ruben and written by Mishna Wolff, based on the video game of the same name from Red Storm Entertainment. It stars Sam Richardson, Milana Vayntrub, George Basil, Sarah Burns, Michael Chernus, Catherine Curtin, Wayne Duvall, Harvey Guillén, Rebecca Henderson, Cheyenne Jackson, Michaela Watkins, and Glenn Fleshler. The film follows a group of people in a small Vermont town who get trapped in a snowstorm while suspecting one of them is a werewolf.\\nPlans for a Werewolves Within film adaptation began in October 2018, with Wolff writing the script and Ubisoft Motion Pictures producing it. The cast was announced in early 2020 and principal photography began in February 2020.\\nWerewolves Within had its world premiere at the Tribeca Film Festival on June 16, 2021, and began a limited theatrical release in the United States on June 25, 2021, followed by video on demand on July 2, by IFC Films. The film received generally positive reviews from critics for its screenplay, characters, and humor, becoming the highest-rated film based on a video game on Rotten Tomatoes and Metacritic.\\n\\nPage: Werewolf Game\\nSummary: Werewolf Game is a 2025 American horror film written by Jackie Payne, directed by Payne and Cara Brennan and starring Tony Todd, Lydia Hearst, Bai Ling, and Robert Picardo.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    }
  ]
}