{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXwlLy/3rPxZlKcJelaG27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enya-yx/LangChain-Courses/blob/main/langgraph_essay_writter_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"langchain-google-genai\" \"langchain\" \"langchain-core\" \"langgraph-prebuilt\" \"google-generativeai\" \"langchain_community\" \"docarray\" \"langchain_experimental\" \"aiosqlite\""
      ],
      "metadata": {
        "id": "sIdyIP6YPtKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('google_api_key')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('tavily_api_key')\n",
        "\n",
        "# Configure the generative AI library with your API key\n",
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n"
      ],
      "metadata": {
        "id": "6gPZp3CQXQzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "#from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Define llm\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0,\n",
        "    verbose=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "UL4c9YZWO0X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict, Annotated\n",
        "import operator\n",
        "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Define a Simple Agent State;\n",
        "# Agent State is accessible to all parts of the graph.\n",
        "class AgentState(TypedDict):\n",
        "  task: str\n",
        "  plan: str\n",
        "  draft: str\n",
        "  critique: str\n",
        "  content: list[str]\n",
        "  revision_number: int\n",
        "  max_revisions: int"
      ],
      "metadata": {
        "id": "EnxFL8gX_nzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tool = TavilySearchResults(max_results=2)\n",
        "#print(tool.name)"
      ],
      "metadata": {
        "id": "3xZ9ErqeMbpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "memory = MemorySaver()\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Queries(BaseModel):\n",
        "  queries: list[str]"
      ],
      "metadata": {
        "id": "G16mz_zHR6dW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. Write such an outine for the user provided topic. Give an outline\\\n",
        "of the essay along with any relevant notes or instructions for the sections.\"\"\"\n",
        "WRITTER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays. Generate the best essay possible for the user's request and\\\n",
        "the initial plan. If the user provides critique, respond with a revised version of your previous attemps. Utilize all the information below as needed:\\\n",
        "------\n",
        "{content}\"\"\"\n",
        "\n",
        "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. Generate critique and recommendations for the user's submission. \\\n",
        "Provide detailed recommendations, including requests for length, depth and so on\"\"\"\n",
        "\n",
        "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can be used when writing the following essay. \\\n",
        "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n",
        "\n",
        "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing critiques to be used when making any requested revisions (as outlined below).\\\n",
        "Generate a list of search queries that will gather any relevant information.\"\"\""
      ],
      "metadata": {
        "id": "9hSFVhkXR5La"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define action nodes\n",
        "def plan_node(state: AgentState):\n",
        "  messages = [\n",
        "      SystemMessage(content=PLAN_PROMPT),\n",
        "      HumanMessage(content=state['task'])\n",
        "  ]\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"plan\": response.content}\n",
        "\n",
        "def research_plan_node(state: AgentState):\n",
        "  queries = llm.with_structured_output(Queries).invoke([\n",
        "      SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
        "      HumanMessage(content=state['task'])\n",
        "  ])\n",
        "  content = state['content'] or []\n",
        "  for q in queries.queries:\n",
        "    response = tool.invoke(q)\n",
        "    for r in response:\n",
        "      content.append(r['content'])\n",
        "  return {\"content\": content}\n",
        "\n",
        "def generation_node(state: AgentState):\n",
        "  content = \"\\n\\n\".join(state['content']) or []\n",
        "  user_message = HumanMessage(\n",
        "      content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
        "  messages = [\n",
        "      SystemMessage(content=WRITTER_PROMPT.format(content=content)\n",
        "      ),\n",
        "      user_message\n",
        "      ]\n",
        "  response = llm.invoke(messages)\n",
        "  return {\n",
        "      \"draft\": response.content,\n",
        "      \"revision_number\": state.get('revision_number', 1) + 1\n",
        "  }\n",
        "def reflection_node(state: AgentState):\n",
        "  messages = [\n",
        "      SystemMessage(content=REFLECTION_PROMPT),\n",
        "      HumanMessage(content=state['draft'])\n",
        "  ]\n",
        "  response = llm.invoke(messages)\n",
        "  return {\"critique\": response.content}\n",
        "\n",
        "def research_critique_node(state: AgentState):\n",
        "  queries = llm.with_structured_output(Queries).invoke([\n",
        "      SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
        "      HumanMessage(content=state['critique'])\n",
        "  ])\n",
        "  content = state['content'] or []\n",
        "  for q in queries.queries:\n",
        "    response = tool.invoke(q)\n",
        "    for r in response:\n",
        "      content.append(r['content'])\n",
        "  return {\"content\": content}"
      ],
      "metadata": {
        "id": "rJ3AcXZ-M33_"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define conditions\n",
        "def should_continue(state):\n",
        "  return \"reflect\" if state['revision_number'] <= state['max_revisions'] else END"
      ],
      "metadata": {
        "id": "putFdLZ9R477"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the graph\n",
        "builder = StateGraph(AgentState)\n",
        "builder.add_node(\"planner\", plan_node)\n",
        "builder.add_node(\"research_plan\", research_plan_node)\n",
        "builder.add_node(\"generate\", generation_node)\n",
        "builder.add_node(\"reflect\", reflection_node)\n",
        "builder.add_node(\"research_critique\", research_critique_node)\n",
        "\n",
        "builder.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    should_continue,\n",
        "    {\"reflect\": \"reflect\", END: END}\n",
        ")\n",
        "builder.set_entry_point(\"planner\")\n",
        "builder.add_edge(\"planner\", \"research_plan\")\n",
        "builder.add_edge(\"research_plan\", \"generate\")\n",
        "builder.add_edge(\"reflect\", \"research_critique\")\n",
        "builder.add_edge(\"research_critique\", \"generate\")\n",
        "graph = builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "XPxouTy_Sn4t"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "for event in graph.stream({\n",
        "      \"task\": \"Write an essay about the impact of AI on human society with the next 5 years\",\n",
        "      \"max_revisions\": 2,\n",
        "      \"revision_number\": 1,\n",
        "      \"content\": []\n",
        "    }, thread):\n",
        "    print(event)"
      ],
      "metadata": {
        "id": "Baf3-SMETlDj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}